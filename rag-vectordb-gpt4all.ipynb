{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764bfbe8-ecf9-49a9-9ec1-107d3a6b7fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"Learning_Python.pdf\") #wget https://images.samsung.com/is/content/samsung/assets/global/ir/docs/2023_con_quarter04_all.pdf\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075422f4-c2c1-443e-b52b-240628c4e4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fe4e8f-6ac6-49a1-a730-9a4328863226",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "local_embedding_model=\"all-MiniLM-L6-v2\" #git clone https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "embeddings = HuggingFaceEmbeddings(model_name=local_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e610ccda-f8ad-41dd-b3ea-798a6fac7c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd8e94a-2db8-41c6-a9b8-719baa1ddbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db = Milvus.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    collection_name=\"Learning_Python\",\n",
    "    connection_args={\"host\": \"local.dev.server\", \"port\": \"19530\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ac6e97-76d7-4fbb-8f11-d15538cea011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44fbfe6-bc89-4c44-aa78-dd880187462d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default collection name - Learning_Python\n",
      "Default search params - {'metric_type': 'L2', 'params': {'ef': 10}}\n",
      "Default index params - None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default collection name - {vector_db.collection_name}\")\n",
    "print(f\"Default search params - {vector_db.search_params}\")\n",
    "print(f\"Default index params - {vector_db.index_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e70268-1b60-4fcf-af1e-7f4aa225332e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='FOURTH EDITION\\nLearning Python\\nMark Lutz\\nBeijing •Cambridge •Farnham •Köln •Sebastopol •Taipei •Tokyo', metadata={'source': 'Learning_Python.pdf', 'page': 3})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6d5b9d-067a-4783-813b-a299d4343b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Collection>:\n",
       "-------------\n",
       "<name>: Learning_Python\n",
       "<description>: \n",
       "<schema>: {'auto_id': True, 'description': '', 'fields': [{'name': 'source', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'page', 'description': '', 'type': <DataType.INT64: 5>}, {'name': 'text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 65535}}, {'name': 'pk', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'enable_dynamic_field': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ab43b6-207c-43fd-8731-d66184bbf05f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 18 key-value pairs and 196 tensors from gpt4all-falcon-newbpe-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = falcon\n",
      "llama_model_loader: - kv   1:                               general.name str              = Falcon\n",
      "llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski\n",
      "llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544\n",
      "llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176\n",
      "llama_model_loader: - kv   6:                         falcon.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71\n",
      "llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = falcon\n",
      "llama_model_loader: - kv   1:                               general.name str              = Falcon\n",
      "llama_model_loader: - kv   2:                      falcon.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                  falcon.tensor_data_layout str              = jploski\n",
      "llama_model_loader: - kv   4:                    falcon.embedding_length u32              = 4544\n",
      "llama_model_loader: - kv   5:                 falcon.feed_forward_length u32              = 18176\n",
      "llama_model_loader: - kv   6:                         falcon.block_count u32              = 32\n",
      "llama_model_loader: - kv   7:                falcon.attention.head_count u32              = 71\n",
      "llama_model_loader: - kv   8:             falcon.attention.head_count_kv u32              = 1\n",
      "llama_model_loader: - kv   9:        falcon.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,65024]   = [\">>TITLE<<\", \">>ABSTRACT<<\", \">>INTR...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,65024]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,65024]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,64784]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"r e\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 11\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_0:  129 tensors\n",
      "llama_model_loader: - type q8_0:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 0\n",
      "llm_load_vocab: token to piece cache size = 0.3884 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = falcon\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 65024\n",
      "llm_load_print_meta: n_merges         = 64784\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4544\n",
      "llm_load_print_meta: n_head           = 71\n",
      "llm_load_print_meta: n_head_kv        = 1\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 71\n",
      "llm_load_print_meta: n_embd_k_gqa     = 64\n",
      "llm_load_print_meta: n_embd_v_gqa     = 64\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18176\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.22 B\n",
      "llm_load_print_meta: model size       = 3.92 GiB (4.66 BPW) \n",
      "llm_load_print_meta: general.name     = Falcon\n",
      "llm_load_print_meta: BOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 11 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 138 'Ä'\n",
      "llm_load_print_meta: EOT token        = 11 '<|endoftext|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4013.48 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   32.00 MiB, K (f16):   16.00 MiB, V (f16):   16.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.25 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    38.79 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1064\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.eos_token_id': '11', 'tokenizer.ggml.model': 'gpt2', 'falcon.attention.layer_norm_epsilon': '0.000010', 'general.file_type': '2', 'falcon.attention.head_count_kv': '1', 'falcon.attention.head_count': '71', 'falcon.block_count': '32', 'falcon.feed_forward_length': '18176', 'falcon.embedding_length': '4544', 'falcon.tensor_data_layout': 'jploski', 'falcon.context_length': '2048', 'general.name': 'Falcon', 'general.architecture': 'falcon'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "local_path = (\n",
    "    \"gpt4all-falcon-newbpe-q4_0.gguf\"  #wget https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf\n",
    ")\n",
    "llm = LlamaCpp(\n",
    "    model_path=local_path,\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    n_ctx=4096,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e436408-6360-48c2-b81c-545bb177e248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: NASA's role in space technology involves developing and testing new technologies for space exploration, such as propulsion systems, spacecraft materials, and communication networks. They also collaborate with other organizations and countries to develop and advance technologies related to space travel and exploration."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2727.86 ms\n",
      "llama_print_timings:      sample time =       8.65 ms /    53 runs   (    0.16 ms per token,  6129.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3011.88 ms /    15 tokens (  200.79 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4951.11 ms /    52 runs   (   95.21 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =    8032.17 ms /    67 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Answer: NASA's role in space technology involves developing and testing new technologies for space exploration, such as propulsion systems, spacecraft materials, and communication networks. They also collaborate with other organizations and countries to develop and advance technologies related to space travel and exploration.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Question: What is NASA's role in Space Technology?\n",
    "\"\"\"\n",
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0228f9-ca9c-46c4-b168-b6148fbc61b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mContext information is below. \n",
      "------------\n",
      "For more details on companies using Python today, see Python’s website at http://www\n",
      ".python.org.\n",
      "What Can I Do with Python?\n",
      "In addition to \n",
      "being a well-designed programming language, Python is useful for ac-\n",
      "complishing real-world tasks—the sorts of things developers do day in and day out.\n",
      "It’s commonly used in a variety of domains, as a tool for scripting other components\n",
      "and implementing standalone programs. In fact, as a general-purpose language,\n",
      "Python’s roles are virtually unlimited: you can use it for everything from website de-\n",
      "velopment and gaming to robotics and spacecraft control.\n",
      "However, the most common Python roles currently seem to fall into a few broad cat-\n",
      "egories. The next few sections describe some of Python’s most common applications\n",
      "today, as well as tools used in each domain. We won’t be able to explore the tools\n",
      "mentioned here in any depth—if you are interested in any of these topics, see the Python\n",
      "website or other resources for more details.\n",
      "Systems Programming\n",
      "Python’s built-in interfaces to operating-system services make it ideal for writing port-\n",
      "able, maintainable system-administration tools and utilities (sometimes called shell\n",
      "tools). Python programs can search files and directory trees, launch other programs, do\n",
      "parallel processing with processes and threads, and so on.\n",
      "Python’s standard library comes with POSIX bindings and support for all the usual OS\n",
      "tools: environment variables, files, sockets, pipes, processes, multiple threads, regular\n",
      "expression pattern matching, command-line arguments, standard stream interfaces,\n",
      "shell-command launchers, filename expansion, and more. In addition, the bulk of Py-\n",
      "thon’s system interfaces are designed to be portable; for example, a script that copies\n",
      "directory trees typically runs unchanged on all major Python platforms. The Stackless\n",
      "Python system, used by EVE Online, also offers advanced solutions to multiprocessing\n",
      "requirements.\n",
      "GUIs\n",
      "Python’s simplicity and rapid turnaround also make it a good match for graphical user\n",
      "interface programming. Python comes with a standard object-oriented interface to the\n",
      "Tk GUI API called tkinter ( Tkinter in 2.6) that allows Python programs to implement\n",
      "portable GUIs with a native look and feel. Python/tkinter GUIs run unchanged on\n",
      "Microsoft Windows, X Windows (on Unix and Linux), and the Mac OS (both Classic\n",
      "and OS X). A free extension package, PMW, adds advanced widgets to the tkinter\n",
      "toolkit. In addition, the wxPython GUI API, based on a C++ library, offers an alternative\n",
      "toolkit for constructing portable GUIs in Python.\n",
      "What Can I Do with Python? | 9\n",
      "------------\n",
      "Given the context information and not prior knowledge, answer the question: What is NASA's role in Space Technology?\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA's role in Space Technology is to develop and operate various spacecraft and technologies used in space exploration. They also collaborate with other agencies and organizations around the world to advance space research and technology."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2727.86 ms\n",
      "llama_print_timings:      sample time =       7.15 ms /    40 runs   (    0.18 ms per token,  5593.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50721.80 ms /   626 tokens (   81.03 ms per token,    12.34 tokens per second)\n",
      "llama_print_timings:        eval time =    3011.17 ms /    39 runs   (   77.21 ms per token,    12.95 tokens per second)\n",
      "llama_print_timings:       total time =   53829.23 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"NASA's role in Space Technology is to develop and operate various spacecraft and technologies used in space exploration. They also collaborate with other agencies and organizations around the world to advance space research and technology.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't find the answer in tne context provided or local db provided, just say that you don't know, don't try to make up an answer from your knowledge apart from the context.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs={\n",
    "        \"refine_prompt\": QA_CHAIN_PROMPT\n",
    "        }\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1}),\n",
    "    return_source_documents=False,\n",
    "    callbacks=None,\n",
    "    chain_type_kwargs={\"refine_prompt\": QA_CHAIN_PROMPT,\"verbose\":True},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "question = \"What is SpaceX's role in Space Technology?\"\n",
    "result = qa_chain({\"query\": question}) # must be query\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0dbca-9474-4061-bb1a-35e3ebe8e38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29b5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7ca65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
